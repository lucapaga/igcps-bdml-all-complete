{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Anatomy of Tensorflow Experiment Class\n",
    "[tf.contrib.learn.Experiment](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "import tensorflow.contrib.metrics as tfmetrics\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Provide an input function\n",
    "TensorFlow Experiments needs a callback function that provides features and labels, and take no input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CSV_COLUMNS  = ('ontime,dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' + \\\n",
    "                ',carrier,dep_lat,dep_lon,arr_lat,arr_lon,origin,dest').split(',')\n",
    "LABEL_COLUMN = 'ontime'\n",
    "DEFAULTS     = [[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],\\\n",
    "                ['na'],[0.0],[0.0],[0.0],[0.0],['na'],['na']]\n",
    "\n",
    "def read_dataset(filename, mode=tf.contrib.learn.ModeKeys.EVAL, batch_size=512, num_training_epochs=10):\n",
    "\n",
    "  # the actual input function passed to TensorFlow\n",
    "  def _input_fn():\n",
    "    num_epochs = num_training_epochs if mode == tf.contrib.learn.ModeKeys.TRAIN else 1\n",
    "\n",
    "    # could be a path to one file or a file pattern.\n",
    "    input_file_names = tf.train.match_filenames_once(filename)\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        input_file_names, num_epochs=num_epochs, shuffle=True)\n",
    " \n",
    "    # read CSV\n",
    "    reader = tf.TextLineReader()\n",
    "    _, value = reader.read_up_to(filename_queue, num_records=batch_size)\n",
    "    value_column = tf.expand_dims(value, -1)\n",
    "    columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n",
    "    features = dict(zip(CSV_COLUMNS, columns))\n",
    "    label = features.pop(LABEL_COLUMN)\n",
    "    return features, label\n",
    "  \n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Provide a model\n",
    "## First: define a helper function\n",
    "Here we can select the features to build models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_features_raw():\n",
    "    real = {\n",
    "      colname : tflayers.real_valued_column(colname) \\\n",
    "          for colname in \\\n",
    "            ('dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' + \n",
    "             ',dep_lat,dep_lon,arr_lat,arr_lon').split(',')\n",
    "    }\n",
    "    sparse = {\n",
    "      'carrier': tflayers.sparse_column_with_keys('carrier',\n",
    "                  keys='AS,VX,F9,UA,US,WN,HA,EV,MQ,DL,OO,B6,NK,AA'.split(',')),\n",
    "      'origin' : tflayers.sparse_column_with_hash_bucket('origin', hash_bucket_size=1000), # FIXME\n",
    "      'dest'   : tflayers.sparse_column_with_hash_bucket('dest', hash_bucket_size=1000) #FIXME\n",
    "    }\n",
    "    return real, sparse\n",
    "\n",
    "def get_features():\n",
    "    return get_features_raw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Wide and Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parse_hidden_units(s):\n",
    "    return [int(item) for item in s.split(',')]\n",
    "\n",
    "def wide_and_deep_model(output_dir, nbuckets=5, hidden_units='64,32', learning_rate=0.01):\n",
    "    real, sparse = get_features()\n",
    "\n",
    "    # the lat/lon columns can be discretized to yield \"air traffic corridors\"\n",
    "    latbuckets = np.linspace(20.0, 50.0, nbuckets).tolist()  # USA\n",
    "    lonbuckets = np.linspace(-120.0, -70.0, nbuckets).tolist() # USA\n",
    "    disc = {}\n",
    "    disc.update({\n",
    "       'd_{}'.format(key) : tflayers.bucketized_column(real[key], latbuckets) \\\n",
    "          for key in ['dep_lat', 'arr_lat']\n",
    "    })\n",
    "    disc.update({\n",
    "       'd_{}'.format(key) : tflayers.bucketized_column(real[key], lonbuckets) \\\n",
    "          for key in ['dep_lon', 'arr_lon']\n",
    "    })\n",
    "\n",
    "    # cross columns that make sense in combination\n",
    "    sparse['dep_loc'] = tflayers.crossed_column([disc['d_dep_lat'], disc['d_dep_lon']],\\\n",
    "                                                nbuckets*nbuckets)\n",
    "    sparse['arr_loc'] = tflayers.crossed_column([disc['d_arr_lat'], disc['d_arr_lon']],\\\n",
    "                                                nbuckets*nbuckets)\n",
    "    sparse['dep_arr'] = tflayers.crossed_column([sparse['dep_loc'], sparse['arr_loc']],\\\n",
    "                                                nbuckets ** 4)\n",
    "    sparse['ori_dest'] = tflayers.crossed_column([sparse['origin'], sparse['dest']], \\\n",
    "                                                hash_bucket_size=1000)\n",
    "    \n",
    "    # create embeddings of all the sparse columns\n",
    "    embed = {\n",
    "       colname : create_embed(col) \\\n",
    "          for colname, col in sparse.items()\n",
    "    }\n",
    "    real.update(embed)\n",
    " \n",
    "    estimator = \\\n",
    "        tflearn.DNNLinearCombinedClassifier(model_dir=output_dir,\n",
    "                                           linear_feature_columns=sparse.values(),\n",
    "                                           dnn_feature_columns=real.values(),\n",
    "                                           dnn_hidden_units=parse_hidden_units(hidden_units))\n",
    "                                           #linear_optimizer=tf.train.FtrlOptimizer(learning_rate=learning_rate),\n",
    "                                           #dnn_optimizer=tf.train.AdagradOptimizer(learning_rate=learning_rate*0.25))\n",
    "    estimator.params[\"head\"]._thresholds = [0.7]  # FIXME: hack\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def linear_model(output_dir):\n",
    "    real, sparse = get_features()\n",
    "    all = {}\n",
    "    all.update(real)\n",
    "    all.update(sparse)\n",
    "    estimator = tflearn.LinearClassifier(model_dir=output_dir, feature_columns=all.values())\n",
    "    estimator.params[\"head\"]._thresholds = [0.7]  # FIXME: hack\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_embed(sparse_col):\n",
    "    dim = 10 # default\n",
    "    if hasattr(sparse_col, 'bucket_size'):\n",
    "       nbins = sparse_col.bucket_size\n",
    "       if nbins is not None:\n",
    "          dim = 1 + int(round(np.log2(nbins)))\n",
    "    return tflayers.embedding_column(sparse_col, dimension=dim)\n",
    "\n",
    "def dnn_model(output_dir):\n",
    "    real, sparse = get_features()\n",
    "    all = {}\n",
    "    all.update(real)\n",
    "\n",
    "    # create embeddings of the sparse columns\n",
    "    embed = {\n",
    "       colname : create_embed(col) \\\n",
    "          for colname, col in sparse.items()\n",
    "    }\n",
    "    all.update(embed)\n",
    "\n",
    "    estimator = tflearn.DNNClassifier(model_dir=output_dir,\n",
    "                                      feature_columns=all.values(),\n",
    "                                      hidden_units=[64, 16, 4])\n",
    "    estimator.params[\"head\"]._thresholds = [0.7]  # FIXME: hack\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Select the actual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_model(output_dir, nbuckets, hidden_units, learning_rate):\n",
    "    #return linear_model(output_dir)\n",
    "    #return dnn_model(output_dir)\n",
    "    return wide_and_deep_model(output_dir, nbuckets, hidden_units, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Provide a function for REST API\n",
    "Inference will be requested on data coming from a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "      key : tf.placeholder(tf.float32, [None]) \\\n",
    "        for key in ('dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' +\n",
    "             ',dep_lat,dep_lon,arr_lat,arr_lon').split(',')\n",
    "    }\n",
    "    feature_placeholders.update( {\n",
    "      key : tf.placeholder(tf.string, [None]) \\\n",
    "        for key in 'carrier,origin,dest'.split(',')\n",
    "    } )\n",
    "\n",
    "    features = {\n",
    "      key: tf.expand_dims(tensor, -1)\n",
    "      for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    return tflearn.utils.input_fn_utils.InputFnOps(\n",
    "      features,\n",
    "      None,\n",
    "      feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Add custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def my_rmse(predictions, labels, **args):\n",
    "  prob_ontime = predictions[:,1]\n",
    "  return tfmetrics.streaming_root_mean_squared_error(prob_ontime, labels, **args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create the Experiment instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_experiment_fn(traindata, evaldata, num_training_epochs,\n",
    "                       batch_size, nbuckets, hidden_units, learning_rate, **args):\n",
    "  def _experiment_fn(output_dir):\n",
    "    return tflearn.Experiment(\n",
    "        get_model(output_dir, nbuckets, hidden_units, learning_rate),\n",
    "        train_input_fn=read_dataset(traindata, mode=tf.contrib.learn.ModeKeys.TRAIN, num_training_epochs=num_training_epochs, batch_size=batch_size),\n",
    "        eval_input_fn=read_dataset(evaldata),\n",
    "        export_strategies=[saved_model_export_utils.make_export_strategy(\n",
    "            serving_input_fn,\n",
    "            default_output_alternative_key=None,\n",
    "            exports_to_keep=1\n",
    "        )],\n",
    "        eval_metrics = {\n",
    "          'rmse' : tflearn.MetricSpec(metric_fn=my_rmse, prediction_key='probabilities'),\n",
    "          'training/hptuning/metric' : tflearn.MetricSpec(metric_fn=my_rmse, prediction_key='probabilities')\n",
    "        },\n",
    "        min_eval_frequency = 100,\n",
    "        **args\n",
    "    )\n",
    "  return _experiment_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a small training session on datalab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = 'telemar-flights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from telemar-flights\n",
      "total 1684\n",
      "-rw-r--r-- 1 root root 749937 Jun  8 15:52 test.csv\n",
      "-rw-r--r-- 1 root root 969431 Jun  8 15:52 train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://telemar-flights/flights/chapter8/output/trainFlights-00001-of-00007.csv...\n",
      "/ [0 files][    0.0 B/108.4 MiB]                                                \r",
      "-\r",
      "- [0 files][ 93.3 MiB/108.4 MiB]                                                \r",
      "- [1 files][108.4 MiB/108.4 MiB]                                                \r",
      "\\\r\n",
      "Operation completed over 1 objects/108.4 MiB.                                    \n",
      "Copying gs://telemar-flights/flights/chapter8/output/testFlights-00001-of-00007.csv...\n",
      "/ [0 files][    0.0 B/732.4 KiB]                                                \r",
      "/ [1 files][732.4 KiB/732.4 KiB]                                                \r\n",
      "Operation completed over 1 objects/732.4 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"reading from $BUCKET\"\n",
    "\n",
    "DATA_DIR=data/flights\n",
    "rm -rf $DATA_DIR\n",
    "mkdir -p $DATA_DIR\n",
    "\n",
    "for STEP in train test; do\n",
    "  gsutil cp gs://$BUCKET/flights/chapter8/output/${STEP}Flights-00001*.csv full.csv\n",
    "  head -10003 full.csv > $DATA_DIR/${STEP}.csv\n",
    "  rm full.csv\n",
    "done\n",
    "\n",
    "ls -l $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f44e7b16f90>, '_model_dir': 'trained_model/', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_save_summary_steps': 100, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_log_step_count_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2 into trained_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.69159335, step = 2\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-08-15:53:10\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_model/model.ckpt-2\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-08-15:53:12\n",
      "INFO:tensorflow:Saving dict for global step 2: accuracy = 0.18103336, accuracy/baseline_label_mean = 0.8189666, accuracy/threshold_0.700000_mean = 0.18103336, auc = 0.5060693, auc_precision_recall = 0.91058207, global_step = 2, labels/actual_label_mean = 0.8189666, labels/prediction_mean = 0.00015233166, loss = 23.813652, precision/positive_threshold_0.700000_mean = 0.0, recall/positive_threshold_0.700000_mean = 0.0, rmse = 0.9048016, training/hptuning/metric = 0.9048016\n",
      "INFO:tensorflow:Validation (step 101): accuracy/baseline_label_mean = 0.8189666, loss = 23.813652, auc = 0.5060693, accuracy/threshold_0.700000_mean = 0.18103336, global_step = 2, rmse = 0.9048016, recall/positive_threshold_0.700000_mean = 0.0, labels/prediction_mean = 0.00015233166, precision/positive_threshold_0.700000_mean = 0.0, training/hptuning/metric = 0.9048016, accuracy = 0.18103336, auc_precision_recall = 0.91058207, labels/actual_label_mean = 0.8189666\n",
      "INFO:tensorflow:global_step/sec: 15.6938\n",
      "INFO:tensorflow:loss = 0.12828864, step = 202 (6.921 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.033\n",
      "INFO:tensorflow:global_step/sec: 200.084\n",
      "INFO:tensorflow:loss = 0.38324958, step = 402 (0.955 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.736\n",
      "INFO:tensorflow:global_step/sec: 216.683\n",
      "INFO:tensorflow:loss = 0.22983252, step = 602 (0.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.579\n",
      "INFO:tensorflow:global_step/sec: 199.246\n",
      "INFO:tensorflow:loss = 0.30969563, step = 802 (0.980 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.347\n",
      "INFO:tensorflow:global_step/sec: 215.854\n",
      "INFO:tensorflow:loss = 0.42414254, step = 1002 (0.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.839\n",
      "INFO:tensorflow:global_step/sec: 216.549\n",
      "INFO:tensorflow:loss = 0.35698944, step = 1202 (0.942 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.96\n",
      "INFO:tensorflow:global_step/sec: 217.48\n",
      "INFO:tensorflow:loss = 0.41152036, step = 1402 (0.926 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.905\n",
      "INFO:tensorflow:global_step/sec: 204.82\n",
      "INFO:tensorflow:loss = 0.50498575, step = 1602 (0.958 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.291\n",
      "INFO:tensorflow:global_step/sec: 225.285\n",
      "INFO:tensorflow:loss = 0.4906544, step = 1802 (0.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.402\n",
      "INFO:tensorflow:global_step/sec: 213.145\n",
      "INFO:tensorflow:loss = 0.47663525, step = 2002 (0.898 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2020 into trained_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.120744675.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-08-15:53:25\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_model/model.ckpt-2020\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-08-15:53:27\n",
      "INFO:tensorflow:Saving dict for global step 2020: accuracy = 0.8189666, accuracy/baseline_label_mean = 0.8189666, accuracy/threshold_0.700000_mean = 0.6219751, auc = 0.53855383, auc_precision_recall = 0.8462137, global_step = 2020, labels/actual_label_mean = 0.8189666, labels/prediction_mean = 0.7377406, loss = 0.493171, precision/positive_threshold_0.700000_mean = 0.826584, recall/positive_threshold_0.700000_mean = 0.6813608, rmse = 0.3957664, training/hptuning/metric = 0.3957664\n",
      "INFO:tensorflow:Restoring parameters from trained_model/model.ckpt-2020\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: trained_model/export/Servo/temp-1528473207/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.8189666,\n",
       "  'accuracy/baseline_label_mean': 0.8189666,\n",
       "  'accuracy/threshold_0.700000_mean': 0.6219751,\n",
       "  'auc': 0.53855383,\n",
       "  'auc_precision_recall': 0.8462137,\n",
       "  'global_step': 2020,\n",
       "  'labels/actual_label_mean': 0.8189666,\n",
       "  'labels/prediction_mean': 0.7377406,\n",
       "  'loss': 0.493171,\n",
       "  'precision/positive_threshold_0.700000_mean': 0.826584,\n",
       "  'recall/positive_threshold_0.700000_mean': 0.6813608,\n",
       "  'rmse': 0.3957664,\n",
       "  'training/hptuning/metric': 0.3957664},\n",
       " ['trained_model/export/Servo/1528473207'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "\n",
    "arguments = {'traindata': 'data/flights/train.csv',\n",
    "             'evaldata': 'data/flights/test.csv',  \n",
    "             'num_training_epochs': 10,\n",
    "             'batch_size': 100,\n",
    "             'nbuckets': 5,  \n",
    "             'hidden_units': '64,64,64,16,4', # Architecture of DNN part of wide-and-deep network\n",
    "             'learning_rate': 0.001 }\n",
    "\n",
    "output_dir = 'trained_model'\n",
    "# when hp-tuning, we need to use different output directories for different runs\n",
    "output_dir = os.path.join(\n",
    "    output_dir,\n",
    "    json.loads(\n",
    "        os.environ.get('TF_CONFIG', '{}')\n",
    "    ).get('task', {}).get('trial', '')\n",
    ")\n",
    " \n",
    "\n",
    "# run\n",
    "tf.reset_default_graph()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "learn_runner.run(make_experiment_fn(**arguments), output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
