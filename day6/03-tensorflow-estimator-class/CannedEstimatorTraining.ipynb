{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Anatomy of Tensorflow Experiment Class\n",
    "[tf.contrib.learn.Experiment](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "import tensorflow.contrib.metrics as tfmetrics\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Provide an input function\n",
    "TensorFlow Experiments needs a callback function that provides features and labels, and takes no input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CSV_COLUMNS  = ('ontime,dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' + \\\n",
    "                ',carrier,dep_lat,dep_lon,arr_lat,arr_lon,origin,dest').split(',')\n",
    "LABEL_COLUMN = 'ontime'\n",
    "DEFAULTS     = [[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],\\\n",
    "                ['na'],[0.0],[0.0],[0.0],[0.0],['na'],['na']]\n",
    "\n",
    "def read_dataset(filename, mode=tf.estimator.ModeKeys.EVAL, batch_size=512, num_training_epochs=10):\n",
    "\n",
    "  # the actual input function passed to TensorFlow\n",
    "  def _input_fn():\n",
    "    num_epochs = num_training_epochs if mode == tf.estimator.ModeKeys.TRAIN else 1\n",
    "\n",
    "    # could be a path to one file or a file pattern.\n",
    "    input_file_names = tf.train.match_filenames_once(filename)\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        input_file_names, num_epochs=num_epochs, shuffle=True)\n",
    " \n",
    "    # read CSV\n",
    "    reader = tf.TextLineReader()\n",
    "    _, value = reader.read_up_to(filename_queue, num_records=batch_size)\n",
    "    value_column = tf.expand_dims(value, -1)\n",
    "    columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n",
    "    features = dict(zip(CSV_COLUMNS, columns))\n",
    "    label = features.pop(LABEL_COLUMN)\n",
    "    return features, label\n",
    "  \n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Provide a model\n",
    "## First: define a helper function\n",
    "Here we can select the features to build models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_features_raw():\n",
    "    real = {\n",
    "      colname : tf.feature_column.numeric_column(colname) \\\n",
    "          for colname in \\\n",
    "            ('dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' + \n",
    "             ',dep_lat,dep_lon,arr_lat,arr_lon').split(',')\n",
    "    }\n",
    "    sparse = {\n",
    "      'carrier': tf.feature_column.categorical_column_with_vocabulary_list('carrier',\n",
    "                  vocabulary_list='AS,VX,F9,UA,US,WN,HA,EV,MQ,DL,OO,B6,NK,AA'.split(','),\n",
    "                  dtype=tf.string)\n",
    "      # NOT CONSIDERED, FOR THE MOMENT\n",
    "      #, 'origin' : tflayers.sparse_column_with_hash_bucket('origin', hash_bucket_size=1000) # FIXME\n",
    "      #, 'dest'   : tflayers.sparse_column_with_hash_bucket('dest', hash_bucket_size=1000) #FIXME\n",
    "    }\n",
    "    return real, sparse\n",
    "\n",
    "def get_features():\n",
    "    return get_features_raw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Wide and Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parse_hidden_units(s):\n",
    "    return [int(item) for item in s.split(',')]\n",
    "\n",
    "def create_embed(sparse_col):\n",
    "    dim = 10 # default\n",
    "    if hasattr(sparse_col, 'bucket_size'):\n",
    "       nbins = sparse_col.bucket_size\n",
    "       if nbins is not None:\n",
    "          dim = 1 + int(round(np.log2(nbins)))\n",
    "    return tf.feature_column.embedding_column(sparse_col, dimension=dim)  \n",
    "  \n",
    "def wide_and_deep_model(output_dir, nbuckets=5, hidden_units='64,32', learning_rate=0.01):\n",
    "    real, sparse = get_features()\n",
    "\n",
    "    # the lat/lon columns can be discretized to yield \"air traffic corridors\"\n",
    "    latbuckets = np.linspace(20.0, 50.0, nbuckets).tolist()  # USA\n",
    "    lonbuckets = np.linspace(-120.0, -70.0, nbuckets).tolist() # USA\n",
    "    disc = {}\n",
    "    disc.update({\n",
    "       'd_{}'.format(key) : tf.feature_column.bucketized_column(real[key], latbuckets) \\\n",
    "          for key in ['dep_lat', 'arr_lat']\n",
    "    })\n",
    "    disc.update({\n",
    "       'd_{}'.format(key) : tf.feature_column.bucketized_column(real[key], lonbuckets) \\\n",
    "          for key in ['dep_lon', 'arr_lon']\n",
    "    })\n",
    "\n",
    "    # cross columns that make sense in combination\n",
    "    # NOT CONSIDERED, FOR THE MOMENT\n",
    "    #sparse['dep_loc'] = tflayers.crossed_column([disc['d_dep_lat'], disc['d_dep_lon']],\\\n",
    "    #                                            nbuckets*nbuckets)\n",
    "    #sparse['arr_loc'] = tflayers.crossed_column([disc['d_arr_lat'], disc['d_arr_lon']],\\\n",
    "    #                                            nbuckets*nbuckets)\n",
    "    #sparse['dep_arr'] = tflayers.crossed_column([sparse['dep_loc'], sparse['arr_loc']],\\\n",
    "    #                                            nbuckets ** 4)\n",
    "    #sparse['ori_dest'] = tflayers.crossed_column([sparse['origin'], sparse['dest']], \\\n",
    "    #                                            hash_bucket_size=1000)\n",
    "    \n",
    "    # create embeddings of all the sparse columns\n",
    "    embed = {\n",
    "       colname : create_embed(col) \\\n",
    "          for colname, col in sparse.items()\n",
    "    }\n",
    "    real.update(embed)\n",
    "    \n",
    "    # updated following: https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/learn/README.md\n",
    "    estimator = \\\n",
    "        tf.estimator.DNNLinearCombinedClassifier(model_dir=output_dir,\n",
    "                                           linear_feature_columns=sparse.values(),\n",
    "                                           dnn_feature_columns=real.values(),\n",
    "                                           dnn_hidden_units=parse_hidden_units(hidden_units),\n",
    "                                           loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE, # see README      \n",
    "                                        )\n",
    "    #linear_optimizer=tf.train.FtrlOptimizer(learning_rate=learning_rate),\n",
    "    #dnn_optimizer=tf.train.AdagradOptimizer(learning_rate=learning_rate*0.25))\n",
    "    \n",
    "    # estimator.params[\"head\"]._thresholds = [0.7]  # FIXME: hack (seems it's not a valid member)\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Select the actual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOT NEEDED AT THE MOMENT\n",
    "\n",
    "def get_model(output_dir, nbuckets, hidden_units, learning_rate):\n",
    "    #return linear_model(output_dir)\n",
    "    #return dnn_model(output_dir)\n",
    "    return wide_and_deep_model(output_dir, nbuckets, hidden_units, learning_rate)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Provide a function for REST API\n",
    "Inference will be requested on data coming from a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# OK with: https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/tensorflow/d_traineval.ipynb\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "      key : tf.placeholder(tf.float32, [None]) \\\n",
    "        for key in ('dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' +\n",
    "             ',dep_lat,dep_lon,arr_lat,arr_lon').split(',')\n",
    "    }\n",
    "    feature_placeholders.update( {\n",
    "      key : tf.placeholder(tf.string, [None]) \\\n",
    "        for key in 'carrier,origin,dest'.split(',')\n",
    "    } )\n",
    "\n",
    "    features = {\n",
    "      key: tf.expand_dims(tensor, -1)\n",
    "      for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    return tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholders)\n",
    "    #return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Add custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def my_rmse(predictions, labels, **args):\n",
    "  prob_ontime = predictions['probabilities'][:,1]\n",
    "  #return tfmetrics.streaming_root_mean_squared_error(prob_ontime, labels, **args)\n",
    "\n",
    "  #pred_values = predictions['predictions']\n",
    "  return {'rmse': tf.metrics.root_mean_squared_error(prob_ontime, labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Run a small training session on datalab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = 'telemar-flights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"reading from $BUCKET\"\n",
    "\n",
    "DATA_DIR=data/flights\n",
    "rm -rf $DATA_DIR\n",
    "mkdir -p $DATA_DIR\n",
    "\n",
    "for STEP in train test; do\n",
    "  gsutil cp gs://$BUCKET/flights/chapter8/output/${STEP}Flights-00001*.csv full.csv\n",
    "  head -10003 full.csv > $DATA_DIR/${STEP}.csv\n",
    "  rm full.csv\n",
    "done\n",
    "\n",
    "ls -l $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/#1529166067462328...\n",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/Servo/#1529166076080610...\n",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/Servo/1529166074/#1529166080229882...\n",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/Servo/1529166074/saved_model.pb#1529166080370508...\n",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/Servo/1529166074/variables/#1529166080552529...\n",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/Servo/1529166074/variables/variables.data-00000-of-00001#1529166080702624...\n",
      "/ [1/17 objects]   5% Done                                                      \r",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/Servo/1529166074/variables/variables.index#1529166080839529...\n",
      "/ [2/17 objects]  11% Done                                                      \r",
      "/ [3/17 objects]  17% Done                                                      \r",
      "/ [4/17 objects]  23% Done                                                      \r",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/checkpoint#1529166068614579...\n",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/eval/events.out.tfevents.1529162671.59f01d715317#1529166073613411...\n",
      "/ [5/17 objects]  29% Done                                                      \r",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/events.out.tfevents.1529162261.59f01d715317#1529166069567133...\n",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/graph.pbtxt#1529166060465789...\n",
      "/ [6/17 objects]  35% Done                                                      \r",
      "/ [7/17 objects]  41% Done                                                      \r",
      "/ [8/17 objects]  47% Done                                                      \r",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/model.ckpt-1.data-00000-of-00001#1529166062836250...\n",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/model.ckpt-1.index#1529166063063083...\n",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/model.ckpt-1.meta#1529166064656326...\n",
      "/ [9/17 objects]  52% Done                                                      \r",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/model.ckpt-200.data-00000-of-00001#1529166067839787...\n",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/model.ckpt-200.index#1529166068109215...\n",
      "Removing gs://telemar-flights/flights/chapter9/output_estimator/model.ckpt-200.meta#1529166069300387...\n",
      "/ [10/17 objects]  58% Done                                                     \r",
      "/ [11/17 objects]  64% Done                                                     \r",
      "/ [12/17 objects]  70% Done                                                     \r",
      "/ [13/17 objects]  76% Done                                                     \r",
      "/ [14/17 objects]  82% Done                                                     \r",
      "/ [15/17 objects]  88% Done                                                     \r",
      "/ [16/17 objects]  94% Done                                                     \r",
      "/ [17/17 objects] 100% Done                                                     \r\n",
      "Operation completed over 17 objects.                                             \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#rm -rf trained_model\n",
    "gsutil -m rm -r gs://$BUCKET/flights/chapter9/output_estimator/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f349ab7b250>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'gs://telemar-flights/flights/chapter9/output_estimator/', '_global_id_in_cluster': 0, '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f349b7c6a90>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'gs://telemar-flights/flights/chapter9/output_estimator/', '_global_id_in_cluster': 0, '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 10 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into gs://telemar-flights/flights/chapter9/output_estimator/model.ckpt.\n",
      "INFO:tensorflow:loss = 17.222872, step = 1\n",
      "INFO:tensorflow:global_step/sec: 97.7531\n",
      "INFO:tensorflow:loss = 0.63396776, step = 101 (1.032 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into gs://telemar-flights/flights/chapter9/output_estimator/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.60346353.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-16-16:35:11\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from gs://telemar-flights/flights/chapter9/output_estimator/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-16-16:35:13\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.9059516, accuracy_baseline = 0.8189666, auc = 0.88671446, auc_precision_recall = 0.9643941, average_loss = 0.57389504, global_step = 200, label/mean = 0.8189666, loss = 0.5744633, precision = 0.94279325, prediction/mean = 0.492609, recall = 0.9423415, rmse = 0.4426683\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'origin': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=string>, 'distance': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'avg_dep_delay': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'dep_delay': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'taxiout': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'arr_lat': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=float32>, 'dep_lon': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=float32>, 'dest': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=string>, 'carrier': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=string>, 'avg_arr_delay': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'dep_lat': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>, 'arr_lon': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'origin': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=string>, 'distance': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'avg_dep_delay': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'dep_delay': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'taxiout': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'arr_lat': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=float32>, 'dep_lon': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=float32>, 'dest': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=string>, 'carrier': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=string>, 'avg_arr_delay': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'dep_lat': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>, 'arr_lon': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'origin': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=string>, 'distance': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'avg_dep_delay': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'dep_delay': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'taxiout': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'arr_lat': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=float32>, 'dep_lon': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=float32>, 'dest': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=string>, 'carrier': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=string>, 'avg_arr_delay': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'dep_lat': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>, 'arr_lon': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from gs://telemar-flights/flights/chapter9/output_estimator/model.ckpt-200\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: gs://telemar-flights/flights/chapter9/output_estimator/Servo/temp-1529166915/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://telemar-flights/flights/chapter9/output_estimator/Servo/1529166915'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "\n",
    "arguments = {'traindata': 'data/flights/train.csv',\n",
    "             'evaldata': 'data/flights/test.csv',  \n",
    "             'num_training_epochs': 10,\n",
    "             'batch_size': 100,\n",
    "             'nbuckets': 5,  \n",
    "             'hidden_units': '64,64,64,16,4', # Architecture of DNN part of wide-and-deep network\n",
    "             'learning_rate': 0.001 }\n",
    "\n",
    "output_dir = 'gs://telemar-flights/flights/chapter9/output_estimator/'\n",
    "# when hp-tuning, we need to use different output directories for different runs\n",
    "output_dir = os.path.join(\n",
    "    output_dir,\n",
    "    json.loads(\n",
    "        os.environ.get('TF_CONFIG', '{}')\n",
    "    ).get('task', {}).get('trial', '')\n",
    ")\n",
    " \n",
    "\n",
    "# run\n",
    "tf.reset_default_graph()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# create estimator\n",
    "estimator = wide_and_deep_model(output_dir, \n",
    "                                arguments['nbuckets'],\n",
    "                                arguments['hidden_units'],\n",
    "                                arguments['learning_rate'])\n",
    "\n",
    "estimator = tf.contrib.estimator.add_metrics(estimator, \n",
    "                                             my_rmse)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=read_dataset(arguments['traindata'], \n",
    "                                                          mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                                                          batch_size=arguments['batch_size'], \n",
    "                                                          num_training_epochs=arguments['num_training_epochs']),\n",
    "                                    max_steps=200) # FOR A QUICK RUN\n",
    "\n",
    "#exporter = tf.estimator.LatestExporter('exporter', \n",
    "#serving_input_receiver_fn=serving_input_fn())\n",
    "\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=read_dataset(arguments['evaldata']),\n",
    "                                  steps = None,\n",
    "                                  start_delay_secs = 1, # start evaluating after N seconds\n",
    "                                  throttle_secs = 10)#,  # evaluate every N seconds\n",
    "#exporters = exporter)\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "estimator.export_savedmodel(os.path.join(output_dir,'Servo'),\n",
    "                            serving_input_receiver_fn=serving_input_fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
