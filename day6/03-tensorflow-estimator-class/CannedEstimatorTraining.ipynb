{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Anatomy of Tensorflow Experiment Class\n",
    "[tf.contrib.learn.Experiment](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "import tensorflow.contrib.metrics as tfmetrics\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = 'telemar-flights'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Provide an input function\n",
    "TensorFlow Experiments needs a callback function that provides features and labels, and takes no input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CSV_COLUMNS  = ('ontime,dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' + \\\n",
    "                ',carrier,dep_lat,dep_lon,arr_lat,arr_lon,origin,dest').split(',')\n",
    "LABEL_COLUMN = 'ontime'\n",
    "DEFAULTS     = [[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],\\\n",
    "                ['na'],[0.0],[0.0],[0.0],[0.0],['na'],['na']]\n",
    "\n",
    "def read_dataset(filename, mode=tf.estimator.ModeKeys.EVAL, batch_size=512, num_training_epochs=10):\n",
    "\n",
    "  # the actual input function passed to TensorFlow\n",
    "  def _input_fn():\n",
    "    num_epochs = num_training_epochs if mode == tf.estimator.ModeKeys.TRAIN else 1\n",
    "\n",
    "    # could be a path to one file or a file pattern.\n",
    "    input_file_names = tf.train.match_filenames_once(filename)\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "        input_file_names, num_epochs=num_epochs, shuffle=True)\n",
    " \n",
    "    # read CSV\n",
    "    reader = tf.TextLineReader()\n",
    "    _, value = reader.read_up_to(filename_queue, num_records=batch_size)\n",
    "    value_column = tf.expand_dims(value, -1)\n",
    "    columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n",
    "    features = dict(zip(CSV_COLUMNS, columns))\n",
    "    label = features.pop(LABEL_COLUMN)\n",
    "    return features, label\n",
    "  \n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Provide a model\n",
    "## First: define a helper function\n",
    "Here we can select the features to build models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_features_raw(origin_file, dest_file):\n",
    "    real = {\n",
    "      colname : tf.feature_column.numeric_column(colname) \\\n",
    "          for colname in \\\n",
    "            ('dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' + \n",
    "             ',dep_lat,dep_lon,arr_lat,arr_lon').split(',')\n",
    "    }\n",
    "    sparse = {\n",
    "      'carrier': tf.feature_column.categorical_column_with_vocabulary_list('carrier',\n",
    "                  vocabulary_list='AS,B6,WN,HA,OO,F9,NK,EV,DL,UA,US,AA,MQ,VX'.split(','),\n",
    "                  dtype=tf.string)\n",
    "      , 'origin': tf.feature_column.categorical_column_with_vocabulary_file('origin',origin_file)\n",
    "      , 'dest'   : tf.feature_column.categorical_column_with_vocabulary_file('dest',dest_file)\n",
    "    }\n",
    "    return real, sparse\n",
    "\n",
    "def get_features(origin_file, dest_file):\n",
    "    return get_features_raw(origin_file, dest_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Wide and Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parse_hidden_units(s):\n",
    "    return [int(item) for item in s.split(',')]\n",
    "\n",
    "def create_embed(sparse_col):\n",
    "    dim = 10 # default\n",
    "    if hasattr(sparse_col, 'bucket_size'):\n",
    "       nbins = sparse_col.bucket_size\n",
    "       if nbins is not None:\n",
    "          dim = 1 + int(round(np.log2(nbins)))\n",
    "    return tf.feature_column.embedding_column(sparse_col, dimension=dim)  \n",
    "  \n",
    "def wide_and_deep_model(output_dir,  origin_file, dest_file, nbuckets=5, hidden_units='64,32', learning_rate=0.01):\n",
    "    real, sparse = get_features(origin_file, dest_file)\n",
    "\n",
    "    # the lat/lon columns can be discretized to yield \"air traffic corridors\"\n",
    "    latbuckets = np.linspace(20.0, 50.0, nbuckets).tolist()  # USA\n",
    "    lonbuckets = np.linspace(-120.0, -70.0, nbuckets).tolist() # USA\n",
    "    disc = {}\n",
    "    disc.update({\n",
    "       'd_{}'.format(key) : tf.feature_column.bucketized_column(real[key], latbuckets) \\\n",
    "          for key in ['dep_lat', 'arr_lat']\n",
    "    })\n",
    "    disc.update({\n",
    "       'd_{}'.format(key) : tf.feature_column.bucketized_column(real[key], lonbuckets) \\\n",
    "          for key in ['dep_lon', 'arr_lon']\n",
    "    })\n",
    "\n",
    "    # cross columns that make sense in combination\n",
    "    sparse['dep_loc'] = tf.feature_column.crossed_column([disc['d_dep_lat'], disc['d_dep_lon']],\\\n",
    "                                                nbuckets*nbuckets)\n",
    "    sparse['dep_loc'] = tf.feature_column.crossed_column([disc['d_dep_lat'], disc['d_dep_lon']],\\\n",
    "                                                nbuckets*nbuckets)\n",
    "    sparse['arr_loc'] = tf.feature_column.crossed_column([disc['d_arr_lat'], disc['d_arr_lon']],\\\n",
    "                                                nbuckets*nbuckets)\n",
    "    sparse['dep_arr'] = tf.feature_column.crossed_column([sparse['dep_loc'], sparse['arr_loc']],\\\n",
    "                                                nbuckets ** 4)\n",
    "    sparse['ori_dest'] = tf.feature_column.crossed_column([sparse['origin'], sparse['dest']], \\\n",
    "                                                hash_bucket_size=1000)\n",
    "    \n",
    "    # create embeddings of all the sparse columns\n",
    "    embed = {\n",
    "       colname : create_embed(col) \\\n",
    "          for colname, col in sparse.items()\n",
    "    }\n",
    "    real.update(embed)\n",
    "    \n",
    "    # updated following: https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/learn/README.md\n",
    "    estimator = \\\n",
    "        tf.estimator.DNNLinearCombinedClassifier(model_dir=output_dir,\n",
    "                                           linear_feature_columns=sparse.values(),\n",
    "                                           dnn_feature_columns=real.values(),\n",
    "                                           dnn_hidden_units=parse_hidden_units(hidden_units),\n",
    "                                           loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE, # see README      \n",
    "                                        )\n",
    "    #linear_optimizer=tf.train.FtrlOptimizer(learning_rate=learning_rate),\n",
    "    #dnn_optimizer=tf.train.AdagradOptimizer(learning_rate=learning_rate*0.25))\n",
    "    \n",
    "    # estimator.params[\"head\"]._thresholds = [0.7]  # FIXME: hack (seems it's not a valid member)\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Select the actual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOT NEEDED AT THE MOMENT\n",
    "\n",
    "def get_model(output_dir, nbuckets, hidden_units, learning_rate):\n",
    "    #return linear_model(output_dir)\n",
    "    #return dnn_model(output_dir)\n",
    "    return wide_and_deep_model(output_dir, nbuckets, hidden_units, learning_rate)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Provide a function for REST API\n",
    "Inference will be requested on data coming from a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# OK with: https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/tensorflow/d_traineval.ipynb\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "      key : tf.placeholder(tf.float32, [None]) \\\n",
    "        for key in ('dep_delay,taxiout,distance,avg_dep_delay,avg_arr_delay' +\n",
    "             ',dep_lat,dep_lon,arr_lat,arr_lon').split(',')\n",
    "    }\n",
    "    feature_placeholders.update( {\n",
    "      key : tf.placeholder(tf.string, [None]) \\\n",
    "        for key in 'carrier,origin,dest'.split(',')\n",
    "    } )\n",
    "\n",
    "    features = {\n",
    "      key: tf.expand_dims(tensor, -1)\n",
    "      for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    return tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholders)\n",
    "    #return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Add custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def my_rmse(predictions, labels, **args):\n",
    "  prob_ontime = predictions['probabilities'][:,1]\n",
    "  #return tfmetrics.streaming_root_mean_squared_error(prob_ontime, labels, **args)\n",
    "\n",
    "  #pred_values = predictions['predictions']\n",
    "  return {'rmse': tf.metrics.root_mean_squared_error(prob_ontime, labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Run a small training session on datalab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"reading from $BUCKET\"\n",
    "\n",
    "DATA_DIR=data/flights\n",
    "rm -rf $DATA_DIR\n",
    "mkdir -p $DATA_DIR\n",
    "\n",
    "for STEP in train test; do\n",
    "  gsutil cp gs://$BUCKET/flights/chapter8/output/${STEP}Flights-00001*.csv full.csv\n",
    "  head -10003 full.csv > $DATA_DIR/${STEP}.csv\n",
    "  rm full.csv\n",
    "done\n",
    "\n",
    "ls -l $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#rm -rf trained_model\n",
    "gsutil -m rm -r gs://$BUCKET/flights/chapter9/output_estimator/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "\n",
    "BUCKET = os.environ['BUCKET']\n",
    "\n",
    "arguments = {'traindata': 'data/flights/train.csv',\n",
    "             'evaldata': 'data/flights/test.csv',\n",
    "             'origin_file': os.path.join('gs://'+BUCKET,'flights/chapter8/keys/origin.txt'),\n",
    "             'dest_file': os.path.join('gs://'+BUCKET,'flights/chapter8/keys/dest.txt'),\n",
    "             'num_training_epochs': 1,\n",
    "             'batch_size': 100,\n",
    "             'nbuckets': 5,  \n",
    "             'hidden_units': '64,64,64,16,4', # Architecture of DNN part of wide-and-deep network\n",
    "             'learning_rate': 0.001 }\n",
    "\n",
    "output_dir = 'gs://telemar-flights/flights/chapter9/output_estimator/'\n",
    "# when hp-tuning, we need to use different output directories for different runs\n",
    "output_dir = os.path.join(\n",
    "    output_dir,\n",
    "    json.loads(\n",
    "        os.environ.get('TF_CONFIG', '{}')\n",
    "    ).get('task', {}).get('trial', '')\n",
    ")\n",
    " \n",
    "\n",
    "# run\n",
    "tf.reset_default_graph()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# create estimator\n",
    "estimator = wide_and_deep_model(output_dir,\n",
    "                                arguments['origin_file'],\n",
    "                                arguments['dest_file'], \n",
    "                                arguments['nbuckets'],\n",
    "                                arguments['hidden_units'],\n",
    "                                arguments['learning_rate'])\n",
    "\n",
    "estimator = tf.contrib.estimator.add_metrics(estimator, \n",
    "                                             my_rmse)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=read_dataset(arguments['traindata'], \n",
    "                                                          mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                                                          batch_size=arguments['batch_size'], \n",
    "                                                          num_training_epochs=arguments['num_training_epochs']))\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=read_dataset(arguments['evaldata']),\n",
    "                                  steps = None,\n",
    "                                  start_delay_secs = 1, # start evaluating after N seconds\n",
    "                                  throttle_secs = 600)#,  # evaluate every N seconds\n",
    "#exporters = exporter)\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "estimator.export_savedmodel(os.path.join(output_dir,'Servo'),\n",
    "                            serving_input_receiver_fn=serving_input_fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
